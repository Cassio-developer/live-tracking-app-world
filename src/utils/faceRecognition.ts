// Verificar se face-api.js est√° dispon√≠vel
declare global {
  interface Window {
    faceapi: any;
  }
}

// Importa√ß√£o condicional
let faceapi: any;
try {
  faceapi = require('face-api.js');
} catch (error) {
  console.warn('face-api.js n√£o est√° dispon√≠vel');
}

export interface FaceDetectionResult {
  success: boolean;
  descriptor?: Float32Array;
  landmarks?: any;
  error?: string;
}

export interface FaceComparisonResult {
  isMatch: boolean;
  confidence: number;
  distance: number;
}

export interface FaceSetupResult {
  success: boolean;
  descriptors: Float32Array[];
  error?: string;
}

/**
 * Carrega os modelos necess√°rios para reconhecimento facial
 * @returns Promise<boolean> - true se carregado com sucesso
 */
export const loadFaceModels = async (): Promise<boolean> => {
  if (!faceapi) {
    console.error('‚ùå face-api.js n√£o est√° dispon√≠vel');
    return false;
  }

  try {

    await Promise.all([
      faceapi.loadTinyFaceDetectorModel('/models'),
      faceapi.loadFaceLandmarkModel('/models'),
      faceapi.loadFaceRecognitionModel('/models')
    ]);


    // Verificar se os modelos foram carregados
    const modelsLoaded = faceapi.nets.tinyFaceDetector.isLoaded &&
      faceapi.nets.faceLandmark68Net.isLoaded &&
      faceapi.nets.faceRecognitionNet.isLoaded;



    return modelsLoaded;
  } catch (error) {
    console.error('‚ùå Erro ao carregar modelos:', error);
    return false;
  }
};

/**
 * Inicia o stream de v√≠deo da c√¢mera
 * @param videoElement - Elemento de v√≠deo HTML
 * @param options - Op√ß√µes de configura√ß√£o da c√¢mera
 * @returns Promise<MediaStream | null>
 */
export const startVideoStream = async (
  videoElement: HTMLVideoElement,
  options: MediaStreamConstraints = {
    video: {
      width: { ideal: 1280, min: 640 },
      height: { ideal: 720, min: 480 },
      facingMode: 'user',
      frameRate: { ideal: 30 }
    }
  }
): Promise<MediaStream | null> => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia(options);
    videoElement.srcObject = stream;

    // Aguardar o v√≠deo estar pronto
    await new Promise<void>((resolve) => {
      videoElement.onloadedmetadata = () => {

        resolve();
      };
    });

    await videoElement.play();

    // Aguardar um pouco mais para garantir que o v√≠deo est√° reproduzindo
    await new Promise(resolve => setTimeout(resolve, 500));

    return stream;
  } catch (error) {
    console.error('‚ùå Erro ao iniciar stream de v√≠deo:', error);
    return null;
  }
};

/**
 * Detecta face em um elemento de v√≠deo
 * @param videoElement - Elemento de v√≠deo HTML
 * @returns Promise<FaceDetectionResult>
 */
export const detectFace = async (videoElement: HTMLVideoElement): Promise<FaceDetectionResult> => {

  if (!faceapi) {
    return {
      success: false,
      error: 'face-api.js n√£o est√° dispon√≠vel'
    };
  }

  if (!videoElement) {
    return {
      success: false,
      error: 'Elemento de v√≠deo √© null'
    };
  }

  try {
    if (!videoElement.videoWidth || !videoElement.videoHeight) {
      return {
        success: false,
        error: 'V√≠deo n√£o est√° pronto'
      };
    }



    // Verificar se o v√≠deo est√° realmente reproduzindo
    if (videoElement.paused || videoElement.ended || videoElement.readyState < 2) {
      return {
        success: false,
        error: 'V√≠deo n√£o est√° reproduzindo'
      };
    }

    // Aguardar um pouco para garantir que o v√≠deo est√° est√°vel
    await new Promise(resolve => setTimeout(resolve, 100));

    const detection = await faceapi.detectSingleFace(
      videoElement,
      new faceapi.TinyFaceDetectorOptions({
        inputSize: 512, // Aumentado para melhor detec√ß√£o
        scoreThreshold: 0.01 // Extremamente baixo para detectar qualquer coisa
      })
    ).withFaceLandmarks().withFaceDescriptor();


    if (detection) {
      return {
        success: true,
        descriptor: detection.descriptor,
        landmarks: detection.landmarks
      };
    } else {

      // Tentar com configura√ß√£o ainda mais sens√≠vel
      try {
        const sensitiveDetection = await faceapi.detectSingleFace(
          videoElement,
          new faceapi.TinyFaceDetectorOptions({
            inputSize: 224,
            scoreThreshold: 0.001 // Extremamente baixo
          })
        ).withFaceLandmarks().withFaceDescriptor();

        if (sensitiveDetection) {
          return {
            success: true,
            descriptor: sensitiveDetection.descriptor,
            landmarks: sensitiveDetection.landmarks
          };
        }
      } catch (sensitiveError) {
      }

      return {
        success: false,
        error: 'Nenhuma face detectada'
      };
    }
  } catch (error) {
    return {
      success: false,
      error: 'Erro na detec√ß√£o facial'
    };
  }
};

/**
 * Compara dois descritores faciais
 * @param descriptor1 - Primeiro descritor facial
 * @param descriptor2 - Segundo descritor facial
 * @param threshold - Limiar de similaridade (padr√£o: 0.6)
 * @returns FaceComparisonResult
 */
export const compareFaces = (
  descriptor1: Float32Array,
  descriptor2: Float32Array,
  threshold: number = 0.6
): FaceComparisonResult => {
  if (!faceapi) {
    return {
      isMatch: false,
      confidence: 0,
      distance: Infinity
    };
  }

  try {
    const distance = faceapi.euclideanDistance(descriptor1, descriptor2);
    const isMatch = distance < threshold;
    const confidence = Math.max(0, 1 - distance);

    return {
      isMatch,
      confidence,
      distance
    };
  } catch (error) {
    console.error('‚ùå Erro na compara√ß√£o facial:', error);
    return {
      isMatch: false,
      confidence: 0,
      distance: Infinity
    };
  }
};

/**
 * Captura m√∫ltiplas amostras faciais para registro
 * @param videoElement - Elemento de v√≠deo HTML
 * @param sampleCount - N√∫mero de amostras (padr√£o: 5)
 * @param intervalMs - Intervalo entre capturas (padr√£o: 1000ms)
 * @returns Promise<FaceSetupResult>
 */
export const captureFaceSamples = async (
  videoElement: HTMLVideoElement,
  sampleCount: number = 5,
  intervalMs: number = 1000
): Promise<FaceSetupResult> => {
  try {
    const descriptors: Float32Array[] = [];

    console.log(`üîÑ Capturando ${sampleCount} amostras faciais...`);

    for (let i = 0; i < sampleCount; i++) {
      const detection = await detectFace(videoElement);

      if (detection.success && detection.descriptor) {
        descriptors.push(detection.descriptor);
        console.log(`‚úÖ Amostra ${i + 1}/${sampleCount} capturada`);

        if (i < sampleCount - 1) {
          await new Promise(resolve => setTimeout(resolve, intervalMs));
        }
      } else {
        console.warn(`‚ö†Ô∏è Falha na captura da amostra ${i + 1}`);
      }
    }

    if (descriptors.length >= Math.ceil(sampleCount * 0.8)) {
      return {
        success: true,
        descriptors
      };
    } else {
      return {
        success: false,
        descriptors: [],
        error: 'Poucas amostras v√°lidas capturadas'
      };
    }
  } catch (error) {
    console.error('‚ùå Erro na captura de amostras:', error);
    return {
      success: false,
      descriptors: [],
      error: 'Erro na captura de amostras'
    };
  }
};

/**
 * Desenha landmarks faciais em um canvas
 * @param canvas - Elemento canvas HTML
 * @param detection - Resultado da detec√ß√£o facial
 * @param displaySize - Tamanho de exibi√ß√£o
 */
export const drawFaceLandmarks = (
  canvas: HTMLCanvasElement,
  detection: FaceDetectionResult,
  displaySize: { width: number; height: number }
): void => {
  // Fun√ß√£o temporariamente desabilitada para evitar erros
  // TODO: Implementar desenho de landmarks corretamente
  console.log('üé® drawFaceLandmarks chamada (desabilitada temporariamente)');
};

/**
 * Verifica se o dispositivo suporta reconhecimento facial
 * @returns boolean
 */
export const isFaceRecognitionSupported = (): boolean => {
  return !!(
    navigator.mediaDevices &&
    navigator.mediaDevices.getUserMedia
  );
};

/**
 * Para o stream de v√≠deo
 * @param stream - Stream de m√≠dia
 */
export const stopVideoStream = (stream: MediaStream | null): void => {
  if (stream) {
    stream.getTracks().forEach(track => track.stop());
    console.log('üõë Stream de v√≠deo parado');
  }
}; 