// Verificar se face-api.js est√° dispon√≠vel
declare global {
  interface Window {
    faceapi: any;
  }
}

// Importa√ß√£o condicional
let faceapi: any;
try {
  faceapi = require('face-api.js');
} catch (error) {
  console.warn('face-api.js n√£o est√° dispon√≠vel');
}

export interface FaceDetectionResult {
  success: boolean;
  descriptor?: Float32Array;
  landmarks?: any;
  error?: string;
}

export interface FaceComparisonResult {
  isMatch: boolean;
  confidence: number;
  distance: number;
}

export interface FaceSetupResult {
  success: boolean;
  descriptors: Float32Array[];
  error?: string;
}

/**
 * Carrega os modelos necess√°rios para reconhecimento facial
 * @returns Promise<boolean> - true se carregado com sucesso
 */
export const loadFaceModels = async (): Promise<boolean> => {
  console.log('üîÑ loadFaceModels chamada, faceapi dispon√≠vel:', !!faceapi);
  
  if (!faceapi) {
    console.error('‚ùå face-api.js n√£o est√° dispon√≠vel');
    return false;
  }

  try {
    console.log('üîÑ Carregando modelos de reconhecimento facial...');
    
    await Promise.all([
      faceapi.loadTinyFaceDetectorModel('/models'),
      faceapi.loadFaceLandmarkModel('/models'),
      faceapi.loadFaceRecognitionModel('/models')
    ]);
    
    console.log('‚úÖ Modelos carregados com sucesso');
    
    // Verificar se os modelos foram carregados
    const modelsLoaded = faceapi.nets.tinyFaceDetector.isLoaded && 
                        faceapi.nets.faceLandmark68Net.isLoaded && 
                        faceapi.nets.faceRecognitionNet.isLoaded;
    
    console.log('üìä Status dos modelos:', {
      tinyFaceDetector: faceapi.nets.tinyFaceDetector.isLoaded,
      faceLandmark68: faceapi.nets.faceLandmark68Net.isLoaded,
      faceRecognition: faceapi.nets.faceRecognitionNet.isLoaded
    });
    
    return modelsLoaded;
  } catch (error) {
    console.error('‚ùå Erro ao carregar modelos:', error);
    return false;
  }
};

/**
 * Inicia o stream de v√≠deo da c√¢mera
 * @param videoElement - Elemento de v√≠deo HTML
 * @param options - Op√ß√µes de configura√ß√£o da c√¢mera
 * @returns Promise<MediaStream | null>
 */
export const startVideoStream = async (
  videoElement: HTMLVideoElement,
  options: MediaStreamConstraints = {
    video: {
      width: { ideal: 1280, min: 640 },
      height: { ideal: 720, min: 480 },
      facingMode: 'user',
      frameRate: { ideal: 30 }
    }
  }
): Promise<MediaStream | null> => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia(options);
    videoElement.srcObject = stream;
    
    // Aguardar o v√≠deo estar pronto
    await new Promise<void>((resolve) => {
      videoElement.onloadedmetadata = () => {
        console.log('üìπ Metadados do v√≠deo carregados:', {
          width: videoElement.videoWidth,
          height: videoElement.videoHeight
        });
        resolve();
      };
    });
    
    await videoElement.play();
    
    // Aguardar um pouco mais para garantir que o v√≠deo est√° reproduzindo
    await new Promise(resolve => setTimeout(resolve, 500));
    
    console.log('‚úÖ Stream de v√≠deo iniciado - Status:', {
      readyState: videoElement.readyState,
      paused: videoElement.paused,
      ended: videoElement.ended,
      currentTime: videoElement.currentTime
    });
    
    return stream;
  } catch (error) {
    console.error('‚ùå Erro ao iniciar stream de v√≠deo:', error);
    return null;
  }
};

/**
 * Detecta face em um elemento de v√≠deo
 * @param videoElement - Elemento de v√≠deo HTML
 * @returns Promise<FaceDetectionResult>
 */
export const detectFace = async (videoElement: HTMLVideoElement): Promise<FaceDetectionResult> => {
  console.log('üîç detectFace chamada, faceapi dispon√≠vel:', !!faceapi);
  
  if (!faceapi) {
    console.log('‚ùå face-api.js n√£o est√° dispon√≠vel');
    return {
      success: false,
      error: 'face-api.js n√£o est√° dispon√≠vel'
    };
  }

  if (!videoElement) {
    console.log('‚ùå Elemento de v√≠deo √© null');
    return {
      success: false,
      error: 'Elemento de v√≠deo √© null'
    };
  }

  try {
    if (!videoElement.videoWidth || !videoElement.videoHeight) {
      console.log('‚ùå V√≠deo n√£o est√° pronto:', { width: videoElement.videoWidth, height: videoElement.videoHeight });
      return {
        success: false,
        error: 'V√≠deo n√£o est√° pronto'
      };
    }

    console.log('üîç Tentando detectar face...', { 
      width: videoElement.videoWidth, 
      height: videoElement.videoHeight,
      readyState: videoElement.readyState,
      paused: videoElement.paused,
      ended: videoElement.ended,
      currentTime: videoElement.currentTime,
      duration: videoElement.duration
    });

    // Verificar se o v√≠deo est√° realmente reproduzindo
    if (videoElement.paused || videoElement.ended || videoElement.readyState < 2) {
      console.log('‚ùå V√≠deo n√£o est√° reproduzindo corretamente');
      return {
        success: false,
        error: 'V√≠deo n√£o est√° reproduzindo'
      };
    }

    // Aguardar um pouco para garantir que o v√≠deo est√° est√°vel
    await new Promise(resolve => setTimeout(resolve, 100));

    console.log('üîç Iniciando detec√ß√£o com TinyFaceDetector...');
    const detection = await faceapi.detectSingleFace(
      videoElement,
      new faceapi.TinyFaceDetectorOptions({
        inputSize: 512, // Aumentado para melhor detec√ß√£o
        scoreThreshold: 0.01 // Extremamente baixo para detectar qualquer coisa
      })
    ).withFaceLandmarks().withFaceDescriptor();

    console.log('üîç Resultado da detec√ß√£o:', detection ? 'Face encontrada' : 'Nenhuma face');

    if (detection) {
      console.log('‚úÖ Face detectada com sucesso!');
      return {
        success: true,
        descriptor: detection.descriptor,
        landmarks: detection.landmarks
      };
    } else {
      console.log('‚ùå Nenhuma face detectada com TinyFaceDetector, tentando configura√ß√£o mais sens√≠vel...');
      
      // Tentar com configura√ß√£o ainda mais sens√≠vel
      try {
        const sensitiveDetection = await faceapi.detectSingleFace(
          videoElement,
          new faceapi.TinyFaceDetectorOptions({
            inputSize: 224,
            scoreThreshold: 0.001 // Extremamente baixo
          })
        ).withFaceLandmarks().withFaceDescriptor();
        
        if (sensitiveDetection) {
          console.log('‚úÖ Face detectada com configura√ß√£o sens√≠vel!');
          return {
            success: true,
            descriptor: sensitiveDetection.descriptor,
            landmarks: sensitiveDetection.landmarks
          };
        }
      } catch (sensitiveError) {
        console.log('‚ùå Configura√ß√£o sens√≠vel tamb√©m falhou:', sensitiveError);
      }
      
      console.log('‚ùå Nenhuma face detectada com nenhuma configura√ß√£o');
      return {
        success: false,
        error: 'Nenhuma face detectada'
      };
    }
  } catch (error) {
    console.error('‚ùå Erro na detec√ß√£o facial:', error);
    return {
      success: false,
      error: 'Erro na detec√ß√£o facial'
    };
  }
};

/**
 * Compara dois descritores faciais
 * @param descriptor1 - Primeiro descritor facial
 * @param descriptor2 - Segundo descritor facial
 * @param threshold - Limiar de similaridade (padr√£o: 0.6)
 * @returns FaceComparisonResult
 */
export const compareFaces = (
  descriptor1: Float32Array,
  descriptor2: Float32Array,
  threshold: number = 0.6
): FaceComparisonResult => {
  if (!faceapi) {
    return {
      isMatch: false,
      confidence: 0,
      distance: Infinity
    };
  }

  try {
    const distance = faceapi.euclideanDistance(descriptor1, descriptor2);
    const isMatch = distance < threshold;
    const confidence = Math.max(0, 1 - distance);

    return {
      isMatch,
      confidence,
      distance
    };
  } catch (error) {
    console.error('‚ùå Erro na compara√ß√£o facial:', error);
    return {
      isMatch: false,
      confidence: 0,
      distance: Infinity
    };
  }
};

/**
 * Captura m√∫ltiplas amostras faciais para registro
 * @param videoElement - Elemento de v√≠deo HTML
 * @param sampleCount - N√∫mero de amostras (padr√£o: 5)
 * @param intervalMs - Intervalo entre capturas (padr√£o: 1000ms)
 * @returns Promise<FaceSetupResult>
 */
export const captureFaceSamples = async (
  videoElement: HTMLVideoElement,
  sampleCount: number = 5,
  intervalMs: number = 1000
): Promise<FaceSetupResult> => {
  try {
    const descriptors: Float32Array[] = [];
    
    console.log(`üîÑ Capturando ${sampleCount} amostras faciais...`);
    
    for (let i = 0; i < sampleCount; i++) {
      const detection = await detectFace(videoElement);
      
      if (detection.success && detection.descriptor) {
        descriptors.push(detection.descriptor);
        console.log(`‚úÖ Amostra ${i + 1}/${sampleCount} capturada`);
        
        if (i < sampleCount - 1) {
          await new Promise(resolve => setTimeout(resolve, intervalMs));
        }
      } else {
        console.warn(`‚ö†Ô∏è Falha na captura da amostra ${i + 1}`);
      }
    }
    
    if (descriptors.length >= Math.ceil(sampleCount * 0.8)) {
      return {
        success: true,
        descriptors
      };
    } else {
      return {
        success: false,
        descriptors: [],
        error: 'Poucas amostras v√°lidas capturadas'
      };
    }
  } catch (error) {
    console.error('‚ùå Erro na captura de amostras:', error);
    return {
      success: false,
      descriptors: [],
      error: 'Erro na captura de amostras'
    };
  }
};

/**
 * Desenha landmarks faciais em um canvas
 * @param canvas - Elemento canvas HTML
 * @param detection - Resultado da detec√ß√£o facial
 * @param displaySize - Tamanho de exibi√ß√£o
 */
export const drawFaceLandmarks = (
  canvas: HTMLCanvasElement,
  detection: FaceDetectionResult,
  displaySize: { width: number; height: number }
): void => {
  // Fun√ß√£o temporariamente desabilitada para evitar erros
  // TODO: Implementar desenho de landmarks corretamente
  console.log('üé® drawFaceLandmarks chamada (desabilitada temporariamente)');
};

/**
 * Verifica se o dispositivo suporta reconhecimento facial
 * @returns boolean
 */
export const isFaceRecognitionSupported = (): boolean => {
  return !!(
    navigator.mediaDevices &&
    navigator.mediaDevices.getUserMedia
  );
};

/**
 * Para o stream de v√≠deo
 * @param stream - Stream de m√≠dia
 */
export const stopVideoStream = (stream: MediaStream | null): void => {
  if (stream) {
    stream.getTracks().forEach(track => track.stop());
    console.log('üõë Stream de v√≠deo parado');
  }
}; 